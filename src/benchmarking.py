from openai import OpenAI
import json
from datasets import load_dataset
import os
from together import Together
import random

#model = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
#client = Together(api_key='TODO')

model = 'gpt-4o'
client = OpenAI(api_key='TODO')

# load dataset, use test split for benchmarking
ds = load_dataset("openai/gsm8k", "main")
dataset = ds['test']
print(dataset['question'][0])
print(dataset['answer'][0])

# randomly pick three training examples, fix seed for reproducibility
random.seed(42)
in_context_examples = random.sample(list(ds['train']), 3)

def generate_solution(problem: str, in_context_examples: list, client) -> str:
    """Generate a solution for the given problem using the Llama3.1 model with in-context learning.

    This function generates a solution for a math problem by prompting the Llama3.1 model. A series of in-context examples is provided to the model to guide it in producing answers in the same format. The function uses a streaming API to receive the model's response chunk by chunk.

    Args:
        problem (str): The math problem to be solved, taken from the GSM8K dataset.
        in_context_examples (list): A list of dictionaries containing example problems and their corresponding solutions. Each dictionary must have the keys 'question' and 'answer'.

    Returns:
        str: The solution generated by the Llama3.1 model, following the same format as the in-context examples provided.
    """

    instruction = "Solve the following problems and provide the solution in the exact same format as shown in the examples. In the final answer behind the hash signs, do not include unit signs."
    context = "\n\n".join([f"{ex['question']}\n{ex['answer']}" for ex in in_context_examples])
    task_prompt = f"{instruction}\n\n{context}\n\n{problem}\n"

    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": task_prompt}],
        stream=True,
    )

    answer = ""
    for chunk in stream:
        answer += chunk.choices[0].delta.content or ""
    
    return answer

# generate and store questions, ground truth answers, and generated answers
problems = []
ground_truth_solutions = []

i = 0
for entry in dataset:
    if i > 3:
        break
    problem = entry['question']  
    answer = entry['answer']  
    
    problems.append(problem)
    ground_truth_solutions.append(answer)
    
    i += 1

solutions = [generate_solution(problem, in_context_examples, client) for problem in problems]

predictions = [
    {"problem": problem, "generated_solution": solution, "ground_truth_solution": ground_truth}
    for problem, solution, ground_truth in zip(problems, solutions, ground_truth_solutions)
]

output_file = f'predictions_{model}.json'
with open(output_file, 'w') as f:
    json.dump(predictions, f, indent=4)